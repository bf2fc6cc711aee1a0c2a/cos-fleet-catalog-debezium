{
  "connector_type" : {
    "id" : "debezium-postgres",
    "kind" : "ConnectorType",
    "href" : "/api/connector_mgmt/v1/kafka_connector_types/debezium-postgres-2.0.1.Final",
    "name" : "Debezium PostgreSQL Connector",
    "version" : "2.0.1.Final",
    "channels" : [ "stable" ],
    "description" : null,
    "labels" : [ "source", "debezium", "postgres", "2.0.1.Final" ],
    "capabilities" : [ "data_shape" ],
    "icon_href" : "http://example.com/images/debezium-postgres-2.0.1.Final.png",
    "schema" : {
      "title" : "Debezium PostgreSQL Connector",
      "required" : [ "topic.prefix", "database.hostname", "database.user", "database.dbname" ],
      "type" : "object",
      "properties" : {
        "topic.prefix" : {
          "title" : "Topic prefix",
          "description" : "Topic prefix that identifies and provides a namespace for the particular database server/cluster is capturing changes. The topic prefix should be unique across all other connectors, since it is used as a prefix for all Kafka topic names that receive events emitted by this connector. Only alphanumeric characters, hyphens, dots and underscores must be accepted.",
          "type" : "string",
          "nullable" : false,
          "x-name" : "topic.prefix",
          "x-category" : "CONNECTION"
        },
        "database.hostname" : {
          "title" : "Hostname",
          "description" : "Resolvable hostname or IP address of the database server.",
          "type" : "string",
          "nullable" : false,
          "x-name" : "database.hostname",
          "x-category" : "CONNECTION"
        },
        "database.port" : {
          "format" : "int32",
          "title" : "Port",
          "description" : "Port of the database server.",
          "default" : 5432,
          "type" : "integer",
          "x-name" : "database.port",
          "x-category" : "CONNECTION"
        },
        "database.user" : {
          "title" : "User",
          "description" : "Name of the database user to be used when connecting to the database.",
          "type" : "string",
          "nullable" : false,
          "x-name" : "database.user",
          "x-category" : "CONNECTION"
        },
        "database.password" : {
          "title" : "Password",
          "description" : "Password of the database user to be used when connecting to the database.",
          "oneOf" : [ {
            "format" : "password",
            "description" : "Password of the database user to be used when connecting to the database.",
            "type" : "string"
          }, {
            "description" : "An opaque reference to the password.",
            "type" : "object",
            "properties" : { },
            "additionalProperties" : true
          } ],
          "x-name" : "database.password",
          "x-category" : "CONNECTION"
        },
        "database.dbname" : {
          "title" : "Database",
          "description" : "The name of the database from which the connector should capture changes",
          "type" : "string",
          "nullable" : false,
          "x-name" : "database.dbname",
          "x-category" : "CONNECTION"
        },
        "slot.name" : {
          "title" : "Slot",
          "description" : "The name of the Postgres logical decoding slot created for streaming changes from a plugin. Defaults to 'debezium",
          "default" : "debezium",
          "type" : "string",
          "x-name" : "slot.name",
          "x-category" : "CONNECTION_ADVANCED_REPLICATION"
        },
        "publication.name" : {
          "title" : "Publication",
          "description" : "The name of the Postgres 10+ publication used for streaming changes from a plugin. Defaults to 'dbz_publication'",
          "default" : "dbz_publication",
          "type" : "string",
          "x-name" : "publication.name",
          "x-category" : "CONNECTION_ADVANCED_REPLICATION"
        },
        "publication.autocreate.mode" : {
          "title" : "Publication Auto Create Mode",
          "description" : "Applies only when streaming changes using pgoutput.Determine how creation of a publication should work, the default is all_tables.DISABLED - The connector will not attempt to create a publication at all. The expectation is that the user has created the publication up-front. If the publication isn't found to exist upon startup, the connector will throw an exception and stop.ALL_TABLES - If no publication exists, the connector will create a new publication for all tables. Note this requires that the configured user has access. If the publication already exists, it will be used. i.e CREATE PUBLICATION <publication_name> FOR ALL TABLES;FILTERED - If no publication exists, the connector will create a new publication for all those tables matchingthe current filter configuration (see table/database include/exclude list properties). If the publication already exists, it will be used. i.e CREATE PUBLICATION <publication_name> FOR TABLE <tbl1, tbl2, etc>",
          "default" : "all_tables",
          "enum" : [ "filtered", "disabled", "all_tables" ],
          "type" : "string",
          "x-name" : "publication.autocreate.mode",
          "x-category" : "CONNECTION_ADVANCED_REPLICATION"
        },
        "schema.include.list" : {
          "format" : "list,regex",
          "title" : "Include Schemas",
          "description" : "The schemas for which events should be captured",
          "type" : "string",
          "x-name" : "schema.include.list",
          "x-category" : "FILTERS"
        },
        "schema.exclude.list" : {
          "format" : "list,regex",
          "title" : "Exclude Schemas",
          "description" : "The schemas for which events must not be captured",
          "type" : "string",
          "x-name" : "schema.exclude.list",
          "x-category" : "FILTERS"
        },
        "table.include.list" : {
          "format" : "list,regex",
          "title" : "Include Tables",
          "description" : "The tables for which changes are to be captured",
          "type" : "string",
          "x-name" : "table.include.list",
          "x-category" : "FILTERS"
        },
        "table.exclude.list" : {
          "format" : "list,regex",
          "title" : "Exclude Tables",
          "description" : "A comma-separated list of regular expressions that match the fully-qualified names of tables to be excluded from monitoring",
          "type" : "string",
          "x-name" : "table.exclude.list",
          "x-category" : "FILTERS"
        },
        "column.include.list" : {
          "format" : "list,regex",
          "title" : "Include Columns",
          "description" : "Regular expressions matching columns to include in change events",
          "type" : "string",
          "x-name" : "column.include.list",
          "x-category" : "FILTERS"
        },
        "column.exclude.list" : {
          "format" : "list,regex",
          "title" : "Exclude Columns",
          "description" : "Regular expressions matching columns to exclude from change events",
          "type" : "string",
          "x-name" : "column.exclude.list",
          "x-category" : "FILTERS"
        },
        "snapshot.mode" : {
          "title" : "Snapshot mode",
          "description" : "The criteria for running a snapshot upon startup of the connector. Options include: 'always' to specify that the connector run a snapshot each time it starts up; 'initial' (the default) to specify the connector can run a snapshot only when no offsets are available for the logical server name; 'initial_only' same as 'initial' except the connector should stop after completing the snapshot and before it would normally start emitting changes;'never' to specify the connector should never run a snapshot and that upon first startup the connector should read from the last position (LSN) recorded by the server; and'exported' deprecated, use 'initial' instead; 'custom' to specify a custom class with 'snapshot.custom_class' which will be loaded and used to determine the snapshot, see docs for more details.",
          "default" : "initial",
          "enum" : [ "always", "never", "initial_only", "initial", "custom" ],
          "type" : "string",
          "x-name" : "snapshot.mode",
          "x-category" : "CONNECTOR_SNAPSHOT"
        },
        "decimal.handling.mode" : {
          "title" : "Decimal Handling",
          "description" : "Specify how DECIMAL and NUMERIC columns should be represented in change events, including: 'precise' (the default) uses java.math.BigDecimal to represent values, which are encoded in the change events using a binary representation and Kafka Connect's 'org.apache.kafka.connect.data.Decimal' type; 'string' uses string to represent values; 'double' represents values using Java's 'double', which may not offer the precision but will be far easier to use in consumers.",
          "default" : "precise",
          "enum" : [ "string", "double", "precise" ],
          "type" : "string",
          "x-name" : "decimal.handling.mode",
          "x-category" : "CONNECTOR"
        },
        "message.key.columns" : {
          "title" : "Columns PK mapping",
          "description" : "A semicolon-separated list of expressions that match fully-qualified tables and column(s) to be used as message key. Each expression must match the pattern '<fully-qualified table name>:<key columns>', where the table names could be defined as (DB_NAME.TABLE_NAME) or (SCHEMA_NAME.TABLE_NAME), depending on the specific connector, and the key columns are a comma-separated list of columns representing the custom key. For any table without an explicit key configuration the table's primary key column(s) will be used as message key. Example: dbserver1.inventory.orderlines:orderId,orderLineId;dbserver1.inventory.orders:id",
          "type" : "string",
          "x-name" : "message.key.columns",
          "x-category" : "CONNECTOR_ADVANCED"
        },
        "query.fetch.size" : {
          "format" : "int32",
          "title" : "Query fetch size",
          "description" : "The maximum number of records that should be loaded into memory while streaming. A value of '0' uses the default JDBC fetch size.",
          "default" : 0,
          "type" : "integer",
          "x-name" : "query.fetch.size",
          "x-category" : "ADVANCED"
        },
        "max.batch.size" : {
          "format" : "int32",
          "title" : "Change event batch size",
          "description" : "Maximum size of each batch of source records. Defaults to 2048.",
          "default" : 2048,
          "type" : "integer",
          "x-name" : "max.batch.size",
          "x-category" : "ADVANCED"
        },
        "max.queue.size" : {
          "format" : "int32",
          "title" : "Change event buffer size",
          "description" : "Maximum size of the queue for change events read from the database log but not yet recorded or forwarded. Defaults to 8192, and should always be larger than the maximum batch size.",
          "default" : 8192,
          "type" : "integer",
          "x-name" : "max.queue.size",
          "x-category" : "ADVANCED"
        },
        "data_shape" : {
          "type" : "object",
          "additionalProperties" : false,
          "properties" : {
            "key" : {
              "title" : "Kafka Message Key Format",
              "description" : "The serialization format for the Kafka message key.",
              "x-name" : "data_shape.key",
              "x-category" : "CONNECTOR",
              "$ref" : "#/$defs/serializer"
            },
            "value" : {
              "title" : "Kafka Message Value Format",
              "description" : "The serialization format for the Kafka message value.",
              "x-name" : "data_shape.value",
              "x-category" : "CONNECTOR",
              "$ref" : "#/$defs/serializer"
            }
          }
        }
      },
      "additionalProperties" : true,
      "x-connector-id" : "postgres",
      "x-version" : "2.0.1.Final",
      "x-className" : "io.debezium.connector.postgresql.PostgresConnector",
      "$defs" : {
        "serializer" : {
          "type" : "string",
          "enum" : [ "JSON", "JSON without schema" ],
          "default" : "JSON"
        }
      }
    }
  },
  "channels" : {
    "stable" : {
      "shard_metadata" : {
        "connector_revision" : 2,
        "operators" : [ {
          "type" : "debezium-connector-operator",
          "version" : "[1.0.0,2.0.0)"
        } ],
        "connector_type" : "source",
        "connector_class" : "io.debezium.connector.postgresql.PostgresConnector",
        "container_image" : "quay.io/rhoas/cos-connector-debezium-postgres@sha256:81b1354c9a52b604715507952929a794f46d391807437bc0bd4d31eaeeba9d2b"
      }
    }
  }
}